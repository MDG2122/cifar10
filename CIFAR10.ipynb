{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPoKWtngNQ08F7UgrvthOP8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"6494897061194e518772a7a020fd3d9d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_dc2af8d231f14d7dbf0e7ab0d548ed4a","IPY_MODEL_abdc49bb3e4546d4a6f9e75be88287ab","IPY_MODEL_a212ebce7b4c4df4b6e9f752b35cfd78"],"layout":"IPY_MODEL_1264ce6b3a2e4a74a526fc361e0325e1"}},"dc2af8d231f14d7dbf0e7ab0d548ed4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_847aa94b51424e8cbc3aa6520b2a18a2","placeholder":"​","style":"IPY_MODEL_72d6d836d13b439aa615a84901041a8d","value":"100%"}},"abdc49bb3e4546d4a6f9e75be88287ab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_6fea532a0e254b99869068319e7fe7e8","max":170498071,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b9db9d55ad154bd4a70e23dd4d20a9eb","value":170498071}},"a212ebce7b4c4df4b6e9f752b35cfd78":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2465f8f720ed470e8108f56d2aa5b710","placeholder":"​","style":"IPY_MODEL_90f729fa5b8c47a8ac2ed2ba0682043d","value":" 170498071/170498071 [00:15&lt;00:00, 14009413.12it/s]"}},"1264ce6b3a2e4a74a526fc361e0325e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"847aa94b51424e8cbc3aa6520b2a18a2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72d6d836d13b439aa615a84901041a8d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6fea532a0e254b99869068319e7fe7e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9db9d55ad154bd4a70e23dd4d20a9eb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"2465f8f720ed470e8108f56d2aa5b710":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90f729fa5b8c47a8ac2ed2ba0682043d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["Guide for the code: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html"],"metadata":{"id":"soRQt-bbGxyK"}},{"cell_type":"markdown","source":["**CNN Model for the CIFAR-10 Dataset**\n","\n","Importing libraries"],"metadata":{"id":"21UbfS84E6pf"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"arxmxcGeEfta","executionInfo":{"status":"ok","timestamp":1673361544958,"user_tz":-60,"elapsed":7316,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"cc7de95e-19b1-41f2-fbfa-55b75c2084c6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting torchinfo\n","  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n","Installing collected packages: torchinfo\n","Successfully installed torchinfo-1.7.1\n"]}],"source":["import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","!pip install torchinfo\n","import torchinfo"]},{"cell_type":"markdown","source":["First, the dataset was loaded using the DataLoader pytorch function and the torchvision library. The 10 classes of the dataset were defined, and the transforms.Compose function was used to apply transformations to the original dataset. This transformations were then normalized. A batch_size of 4 was initially selected but then it was changed to 16 since it produced better results."],"metadata":{"id":"siVZiBZVE-CC"}},{"cell_type":"code","source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","batch_size = 16\n","\n","trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n","                                          shuffle=True, num_workers=2)\n","\n","testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n","                                       download=True, transform=transform)\n","testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n","                                         shuffle=False, num_workers=2)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["6494897061194e518772a7a020fd3d9d","dc2af8d231f14d7dbf0e7ab0d548ed4a","abdc49bb3e4546d4a6f9e75be88287ab","a212ebce7b4c4df4b6e9f752b35cfd78","1264ce6b3a2e4a74a526fc361e0325e1","847aa94b51424e8cbc3aa6520b2a18a2","72d6d836d13b439aa615a84901041a8d","6fea532a0e254b99869068319e7fe7e8","b9db9d55ad154bd4a70e23dd4d20a9eb","2465f8f720ed470e8108f56d2aa5b710","90f729fa5b8c47a8ac2ed2ba0682043d"]},"id":"a-qt8w2GE-L7","executionInfo":{"status":"ok","timestamp":1673361565073,"user_tz":-60,"elapsed":20121,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"79339b46-1e13-424a-92b2-f817c494a649"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/170498071 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6494897061194e518772a7a020fd3d9d"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Extracting ./data/cifar-10-python.tar.gz to ./data\n","Files already downloaded and verified\n"]}]},{"cell_type":"markdown","source":["**CNN Model**\n","\n","For the CNN model, the following arquitecture was used, in this order: \n","\n","---The first part of the architecture consists of 3 blocks with the following structure:\n","\n","-2 Convolutional layers: Starting with 3 channels, each convolutional layer increased the number of channels to 16, 32, 64, 128, 256 and finally 512 channels. All convolutional layers have a 3x3 Filter.\n","\n","-Batch Normalization: Normalizes data at the end of each convolution layer, and gives the network more stability when training, since all values and weights have the same scale.\n","\n","-Activation Function: Then, ReLU activation is used, which is a non-linear activation function, for the output of the batch normalization.\n","\n","-Max Pooling Layer: The max pooling layer downsamples the size of the image. For the first block, a 2x2 filter with 2 stride was used, and for the next 2 blocks, a 1x1 filter with 1 stride was selected.\n","\n","-Dropout Layer: At the end of each block, a 50% dropout layer is applied.\n","\n","---Then, after the 3 CNN blocks, the features are given to the prediction model (second part of the architecture):\n","\n","-Flatten Layer: The output of the last block is flattened (except for the batch) to a size of 18432 features (6x6x512).\n","\n","-Dense Layer: This first dense layer processes the features extracted from the CNN blocks and gives an output of 1024 features. \n","\n","-Activation Function: A ReLU activation function is used for the Dense layer.\n","\n","-Dense Layer: This second layer further processes the features and gives an output of 256 features.\n","\n","-Activation Function: A ReLU activation function is used for the Dense layer.\n","\n","-Output Layer: This layer gives the final output of the model an gives an output of 10 features that correspond to each of the 10 classes. A linear activation is used on this final layer."],"metadata":{"id":"31bl1__6E-TP"}},{"cell_type":"code","source":["class Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(3, 16, 3)\n","        self.pool1 = nn.MaxPool2d(2, 2)\n","        self.norm1 = nn.BatchNorm2d(16)\n","        self.norm2 = nn.BatchNorm2d(32)\n","        self.norm3 = nn.BatchNorm2d(64)\n","        self.norm4 = nn.BatchNorm2d(128)\n","        self.norm5 = nn.BatchNorm2d(256)\n","        self.norm6 = nn.BatchNorm2d(512)\n","        self.drop = nn.Dropout(0.5)\n","        self.conv2 = nn.Conv2d(16, 32, 3)\n","        self.pool2 = nn.MaxPool2d(1, 1)\n","        self.conv3 = nn.Conv2d(32, 64, 3)\n","        self.conv4 = nn.Conv2d(64, 128, 3)\n","        self.conv5 = nn.Conv2d(128, 256, 3)\n","        self.conv6 = nn.Conv2d(256, 512, 3)\n","        self.fc1 = nn.Linear(512 * 6 * 6, 1024)\n","        self.fc2 = nn.Linear(1024, 256)\n","        self.fc3 = nn.Linear(256, 10)\n","\n","    def forward(self, x):\n","        x  = F.relu(self.norm1(self.conv1(x)))\n","        x = F.relu(self.norm2(self.conv2(x)))\n","        x = self.pool1(x)\n","        x = self.drop(x)\n","        x = F.relu(self.norm3(self.conv3(x)))\n","        x = F.relu(self.norm4(self.conv4(x)))\n","        x = self.pool2(x)\n","        x = self.drop(x)\n","        x = F.relu(self.norm5(self.conv5(x)))\n","        x = F.relu(self.norm6(self.conv6(x)))\n","        x = self.pool2(x)\n","        x = self.drop(x)\n","        x = torch.flatten(x, 1) # flatten all dimensions except batch\n","        x = F.relu(self.fc1(x))\n","        x = self.drop(x)\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n","\n","\n","net = Net()"],"metadata":{"id":"YWM2abzYE_4Z","executionInfo":{"status":"ok","timestamp":1673361565536,"user_tz":-60,"elapsed":4,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["Summary of the model outputs and parameters for each layer, and the total number of trainable parameters in the model."],"metadata":{"id":"UI-1pdBvFAAC"}},{"cell_type":"code","source":["torchinfo.summary(Net(), [(3, 32, 32)], batch_dim = 0, verbose = 0)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7syn7PEhFAHX","executionInfo":{"status":"ok","timestamp":1673361578371,"user_tz":-60,"elapsed":12838,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"f5b1fa7e-d98a-4bb4-9ed4-fdfad8bdd3c1"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["==========================================================================================\n","Layer (type:depth-idx)                   Output Shape              Param #\n","==========================================================================================\n","Net                                      [1, 10]                   --\n","├─Conv2d: 1-1                            [1, 16, 30, 30]           448\n","├─BatchNorm2d: 1-2                       [1, 16, 30, 30]           32\n","├─Conv2d: 1-3                            [1, 32, 28, 28]           4,640\n","├─BatchNorm2d: 1-4                       [1, 32, 28, 28]           64\n","├─MaxPool2d: 1-5                         [1, 32, 14, 14]           --\n","├─Dropout: 1-6                           [1, 32, 14, 14]           --\n","├─Conv2d: 1-7                            [1, 64, 12, 12]           18,496\n","├─BatchNorm2d: 1-8                       [1, 64, 12, 12]           128\n","├─Conv2d: 1-9                            [1, 128, 10, 10]          73,856\n","├─BatchNorm2d: 1-10                      [1, 128, 10, 10]          256\n","├─MaxPool2d: 1-11                        [1, 128, 10, 10]          --\n","├─Dropout: 1-12                          [1, 128, 10, 10]          --\n","├─Conv2d: 1-13                           [1, 256, 8, 8]            295,168\n","├─BatchNorm2d: 1-14                      [1, 256, 8, 8]            512\n","├─Conv2d: 1-15                           [1, 512, 6, 6]            1,180,160\n","├─BatchNorm2d: 1-16                      [1, 512, 6, 6]            1,024\n","├─MaxPool2d: 1-17                        [1, 512, 6, 6]            --\n","├─Dropout: 1-18                          [1, 512, 6, 6]            --\n","├─Linear: 1-19                           [1, 1024]                 18,875,392\n","├─Dropout: 1-20                          [1, 1024]                 --\n","├─Linear: 1-21                           [1, 256]                  262,400\n","├─Linear: 1-22                           [1, 10]                   2,570\n","==========================================================================================\n","Total params: 20,715,146\n","Trainable params: 20,715,146\n","Non-trainable params: 0\n","Total mult-adds (M): 94.61\n","==========================================================================================\n","Input size (MB): 0.01\n","Forward/backward pass size (MB): 1.55\n","Params size (MB): 82.86\n","Estimated Total Size (MB): 84.42\n","=========================================================================================="]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["Loss function and opttimizer:\n","\n","-For the loss function, Cross-Entropy was selected since this is a multi-class classification problem.\n","\n","-For the optimizer, 2 different optimizers were tested: Adam and Stochastic Gradient Descent (SGD). SGD performed better than Adam, with a learning rate of 1e-3 and momentum of 0.9."],"metadata":{"id":"Gveq9rBWFAOs"}},{"cell_type":"code","source":["criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"],"metadata":{"id":"aFr6vmv4FAUy","executionInfo":{"status":"ok","timestamp":1673361578372,"user_tz":-60,"elapsed":34,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Checking if CUDA GPUs are available, if not, CPU is used."],"metadata":{"id":"-js90ax7FAa0"}},{"cell_type":"code","source":["device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","\n","print(device)\n","\n","net.to(device)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"s0SHrOfdFAhN","executionInfo":{"status":"ok","timestamp":1673361663231,"user_tz":-60,"elapsed":5,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"fd81b72e-9206-4998-97c4-8aa939dfae8c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]},{"output_type":"execute_result","data":{"text/plain":["Net(\n","  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (norm1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (norm2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (norm3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (norm5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (norm6): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (drop): Dropout(p=0.5, inplace=False)\n","  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n","  (pool2): MaxPool2d(kernel_size=1, stride=1, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n","  (conv4): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n","  (conv5): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1))\n","  (conv6): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1))\n","  (fc1): Linear(in_features=18432, out_features=1024, bias=True)\n","  (fc2): Linear(in_features=1024, out_features=256, bias=True)\n","  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",")"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["**Training of the model**\n","\n","The model was trained for 50 epochs, the input is processed by the model, then the loss function is updated and the Backward pass is computed. The training loss is printed every 2000 processed training patterns (images)."],"metadata":{"id":"eGACc4hpFiKT"}},{"cell_type":"code","source":["for epoch in range(50):  # loop over the dataset multiple times\n","\n","    running_loss = 0.0\n","    for i, data in enumerate(trainloader, 0):\n","        # get the inputs; data is a list of [inputs, labels]\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = net(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 2000 == 1999:    # print every 2000 mini-batches\n","            print(f'[{epoch + 1}, {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n","            running_loss = 0.0\n","\n","print('Finished Training')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fG2oLejpFiR6","executionInfo":{"status":"ok","timestamp":1673363740740,"user_tz":-60,"elapsed":2064057,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"d8828a46-c689-422d-d245-6a9b1825a7d3"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["[1,  2000] loss: 1.737\n","[2,  2000] loss: 1.378\n","[3,  2000] loss: 1.197\n","[4,  2000] loss: 1.088\n","[5,  2000] loss: 1.000\n","[6,  2000] loss: 0.932\n","[7,  2000] loss: 0.879\n","[8,  2000] loss: 0.835\n","[9,  2000] loss: 0.793\n","[10,  2000] loss: 0.763\n","[11,  2000] loss: 0.736\n","[12,  2000] loss: 0.713\n","[13,  2000] loss: 0.681\n","[14,  2000] loss: 0.659\n","[15,  2000] loss: 0.643\n","[16,  2000] loss: 0.625\n","[17,  2000] loss: 0.616\n","[18,  2000] loss: 0.592\n","[19,  2000] loss: 0.576\n","[20,  2000] loss: 0.554\n","[21,  2000] loss: 0.555\n","[22,  2000] loss: 0.534\n","[23,  2000] loss: 0.523\n","[24,  2000] loss: 0.509\n","[25,  2000] loss: 0.500\n","[26,  2000] loss: 0.486\n","[27,  2000] loss: 0.472\n","[28,  2000] loss: 0.466\n","[29,  2000] loss: 0.459\n","[30,  2000] loss: 0.446\n","[31,  2000] loss: 0.435\n","[32,  2000] loss: 0.427\n","[33,  2000] loss: 0.415\n","[34,  2000] loss: 0.410\n","[35,  2000] loss: 0.403\n","[36,  2000] loss: 0.394\n","[37,  2000] loss: 0.388\n","[38,  2000] loss: 0.377\n","[39,  2000] loss: 0.364\n","[40,  2000] loss: 0.365\n","[41,  2000] loss: 0.355\n","[42,  2000] loss: 0.346\n","[43,  2000] loss: 0.346\n","[44,  2000] loss: 0.342\n","[45,  2000] loss: 0.331\n","[46,  2000] loss: 0.328\n","[47,  2000] loss: 0.316\n","[48,  2000] loss: 0.306\n","[49,  2000] loss: 0.308\n","[50,  2000] loss: 0.296\n","Finished Training\n"]}]},{"cell_type":"markdown","source":["**Testing of the model**\n","\n","The model is tested on 10000 images from the CIFAR-10 Dataset. The class that corresponds to the highest output value of the model is selected as the prediction. Then, all correct values are added, divided by the total and multiplied by 100%, to obtain the test accuracy. The final result achieved with this model is 78% accuracy."],"metadata":{"id":"RKq_dzfpFiYr"}},{"cell_type":"code","source":["correct = 0\n","total = 0\n","# since we're not training, we don't need to calculate the gradients for our outputs\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data[0].to(device), data[1].to(device)\n","        # calculate outputs by running images through the network\n","        outputs = net(images)\n","        # the class with the highest energy is what we choose as prediction\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum().item()\n","\n","print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"m7GDxI_XFie7","executionInfo":{"status":"ok","timestamp":1673363744015,"user_tz":-60,"elapsed":3304,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"3b35515e-f672-49fa-ea1a-507bbf867bda"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy of the network on the 10000 test images: 78 %\n"]}]},{"cell_type":"markdown","source":["For each class, the accuracy is calculated with the already trained model."],"metadata":{"id":"1v1ZoIQaHfti"}},{"cell_type":"code","source":["# prepare to count predictions for each class\n","correct_pred = {classname: 0 for classname in classes}\n","total_pred = {classname: 0 for classname in classes}\n","\n","# again no gradients needed\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data[0].to(device), data[1].to(device)\n","        outputs = net(images)\n","        _, predictions = torch.max(outputs, 1)\n","        # collect the correct predictions for each class\n","        for label, prediction in zip(labels, predictions):\n","            if label == prediction:\n","                correct_pred[classes[label]] += 1\n","            total_pred[classes[label]] += 1\n","\n","\n","# print accuracy for each class\n","for classname, correct_count in correct_pred.items():\n","    accuracy = 100 * float(correct_count) / total_pred[classname]\n","    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"srCVx-88FirX","executionInfo":{"status":"ok","timestamp":1673363748879,"user_tz":-60,"elapsed":4869,"user":{"displayName":"Massimo Di Gennaro","userId":"15460351046689668665"}},"outputId":"2276e968-3eda-465c-9e14-81390da0d775"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy for class: plane is 79.2 %\n","Accuracy for class: car   is 89.3 %\n","Accuracy for class: bird  is 67.2 %\n","Accuracy for class: cat   is 63.0 %\n","Accuracy for class: deer  is 78.3 %\n","Accuracy for class: dog   is 64.7 %\n","Accuracy for class: frog  is 81.0 %\n","Accuracy for class: horse is 81.5 %\n","Accuracy for class: ship  is 82.9 %\n","Accuracy for class: truck is 88.5 %\n"]}]},{"cell_type":"markdown","source":["The following results were obtained:\n","\n","Accuracy for class: plane is 79.2 %\n","\n","Accuracy for class: car   is 89.3 %\n","\n","Accuracy for class: bird  is 67.2 %\n","\n","Accuracy for class: cat   is 63.0 %\n","\n","Accuracy for class: deer  is 78.3 %\n","\n","Accuracy for class: dog   is 64.7 %\n","\n","Accuracy for class: frog  is 81.0 %\n","\n","Accuracy for class: horse is 81.5 %\n","\n","Accuracy for class: ship  is 82.9 %\n","\n","Accuracy for class: truck is 88.5 %\n","\n","In conclusion, it is easier for the model to predict objects like cars and ships, but lacks accuracy when predicting animals like cats."],"metadata":{"id":"hLqyMvhrFilc"}}]}